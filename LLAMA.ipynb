{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e583aa43",
      "metadata": {
        "id": "e583aa43"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad3a2c54",
      "metadata": {
        "id": "ad3a2c54"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U transformers\n",
        "%pip install -U datasets\n",
        "%pip install -U accelerate\n",
        "%pip install -U peft\n",
        "%pip install -U trl\n",
        "%pip install -U bitsandbytes\n",
        "%pip install -U wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccafde34",
      "metadata": {
        "id": "ccafde34"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model,\n",
        ")\n",
        "import os, torch, wandb\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, setup_chat_format"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "401c3e4c",
      "metadata": {
        "id": "401c3e4c"
      },
      "source": [
        "## Hugging face authorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba0416c3",
      "metadata": {
        "id": "ba0416c3"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7357fd4d",
      "metadata": {
        "id": "7357fd4d"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"data.json\"\n",
        "new_model = \"llama-3-8b-chat-vitv1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e29f3f",
      "metadata": {
        "id": "80e29f3f"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29af86b7",
      "metadata": {
        "id": "29af86b7"
      },
      "outputs": [],
      "source": [
        "torch_dtype = torch.float16\n",
        "attn_implementation = \"eager\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac19a2a",
      "metadata": {
        "id": "fac19a2a"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e5cdab6",
      "metadata": {
        "id": "8e5cdab6"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = setup_chat_format(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sww8xTDJ9kYi"
      },
      "id": "sww8xTDJ9kYi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfadbc75",
      "metadata": {
        "id": "dfadbc75"
      },
      "outputs": [],
      "source": [
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68815cdb",
      "metadata": {
        "id": "68815cdb"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"json\", data_files=\"data.json\", split=\"all\")\n",
        "\n",
        "def format_chat_template(row):\n",
        "    # Update to use 'question' and 'answer' keys\n",
        "    row_json = [{\"role\": \"user\", \"content\": row[\"question\"]},\n",
        "                {\"role\": \"assistant\", \"content\": row[\"answer\"]}]\n",
        "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    num_proc=4,\n",
        ")\n",
        "\n",
        "dataset # Access formatted text from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ac91f5",
      "metadata": {
        "id": "67ac91f5"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"VIT-BOT\")"
      ],
      "metadata": {
        "id": "47uJYA-tOFTF"
      },
      "id": "47uJYA-tOFTF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d698e46",
      "metadata": {
        "id": "2d698e46"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=new_model,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    num_train_epochs=7,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    logging_steps=1,\n",
        "    warmup_steps=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    learning_rate=1e-4,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    group_by_length=True,\n",
        "    report_to=\"wandb\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ca4c670",
      "metadata": {
        "id": "3ca4c670"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5ee902b",
      "metadata": {
        "id": "d5ee902b"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=512,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing= False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e54d891",
      "metadata": {
        "id": "8e54d891"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "qG7dUdL2qeMz"
      },
      "id": "qG7dUdL2qeMz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a535becf",
      "metadata": {
        "id": "a535becf"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Tell me about VIT's leadership team?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False,\n",
        "                                       add_generation_prompt=True)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt', padding=True,\n",
        "                   truncation=True).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=100,\n",
        "                         num_return_sequences=1)\n",
        "\n",
        "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(text.split(\"assistant\")[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.save_pretrained(new_model)\n",
        "trainer.model.push_to_hub(new_model, use_temp_dir=False)"
      ],
      "metadata": {
        "id": "4xZ8fd83A5t_"
      },
      "id": "4xZ8fd83A5t_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and infer"
      ],
      "metadata": {
        "id": "3RFeyiWMqqAA"
      },
      "id": "3RFeyiWMqqAA"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "sr5jNHj3AGR8"
      },
      "id": "sr5jNHj3AGR8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"casarulez/merged-vit-bot\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"casarulez/merged-vit-bot\")"
      ],
      "metadata": {
        "id": "QE42Q5P3qrao"
      },
      "id": "QE42Q5P3qrao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "VBFVDFqs3T-q"
      },
      "id": "VBFVDFqs3T-q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Who is Viswanathan?\"\n",
        "    }\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False,\n",
        "                                       add_generation_prompt=True)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt', padding=True,\n",
        "                   truncation=True).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=100,\n",
        "                         num_return_sequences=1)\n",
        "\n",
        "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(text.split(\"assistant\")[1])"
      ],
      "metadata": {
        "id": "pW06ND9t_bjL"
      },
      "id": "pW06ND9t_bjL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processed outputs"
      ],
      "metadata": {
        "id": "fHAjt6q6Tz9F"
      },
      "id": "fHAjt6q6Tz9F"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unwanted_content(response):\n",
        "    # Convert the response to lowercase for case-insensitive search\n",
        "    lower_response = response.lower()\n",
        "\n",
        "    # Find the position of the first occurrence of \"question\" or \"what\"\n",
        "    first_question_index = lower_response.find(\"question\")\n",
        "    first_what_index = lower_response.find(\"what\")\n",
        "\n",
        "    # Find the second occurrence of \"question\" or \"what\"\n",
        "    second_question_index = lower_response.find(\"question\", first_question_index + len(\"question\")) if first_question_index != -1 else -1\n",
        "    second_what_index = lower_response.find(\"what\", first_what_index + len(\"what\")) if first_what_index != -1 else -1\n",
        "\n",
        "    # Determine which of the two occurs second (question or what)\n",
        "    if second_question_index != -1 and second_what_index != -1:\n",
        "        second_occurrence_index = min(second_question_index, second_what_index)\n",
        "    elif second_question_index != -1:\n",
        "        second_occurrence_index = second_question_index\n",
        "    elif second_what_index != -1:\n",
        "        second_occurrence_index = second_what_index\n",
        "    else:\n",
        "        # If no second occurrence, return the response as is\n",
        "        processed_response = response.strip()\n",
        "        return processed_response\n",
        "\n",
        "    # Keep the content up to the second occurrence and remove everything after it\n",
        "    processed_response = response[:second_occurrence_index].strip()\n",
        "\n",
        "    return processed_response"
      ],
      "metadata": {
        "id": "gqs6gKKXCHXE"
      },
      "id": "gqs6gKKXCHXE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are VIT-BOT, a virtual assistant designed to answer student questions \"\n",
        "            \"regarding university policies, academic procedures, and campus facilities at VIT. \"\n",
        "            \"Answer each prompt concisely and directly, focusing only on university-relevant information. \"\n",
        "            \"Provide only the answer to the current query without adding any follow-up or additional information.\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What are honours and minors at VIT?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "user_input = messages[1][\"content\"].strip().lower()\n",
        "\n",
        "\n",
        "predefined_responses = {\n",
        "    \"who are you?\": \"I am VIT-BOT, your personal chat assistant to provide you information about the university.\",\n",
        "    \"what can you do?\": \"I can help you with any queries regarding VIT\"\n",
        "}\n",
        "\n",
        "\n",
        "if user_input in predefined_responses:\n",
        "    response = predefined_responses[user_input]\n",
        "else:\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "\n",
        "    if tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
        "        attention_mask = (inputs[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
        "        inputs[\"attention_mask\"] = attention_mask\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=150,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "cleaned_response = remove_unwanted_content(response)\n",
        "\n",
        "print(cleaned_response)\n"
      ],
      "metadata": {
        "id": "rj4h0jTORi2V"
      },
      "id": "rj4h0jTORi2V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bk96Zo8T4RBo"
      },
      "id": "bk96Zo8T4RBo"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}